# Research Assistant API  
### Retrieval-Augmented Generation (RAG) over arXiv

A research-oriented Retrieval-Augmented Generation (RAG) API for querying **recent machine learning and AI papers from arXiv**, with a strong focus on **grounded answers, transparent retrieval, and reproducibility**.

This project is designed as a **technical portfolio piece** demonstrating end-to-end RAG system design, including ingestion, hybrid retrieval, cross-encoder reranking, and recency-aware ranking.



## Overview

The system ingests recent arXiv papers, indexes them at **chunk level**, and answers user queries by:

- retrieving relevant paper chunks,
- re-ranking results using a **cross-encoder**,
- optionally favoring **recent publications**,
- generating **answers strictly grounded in retrieved context**.

All answers are explicitly linked to the source papers to avoid hallucinations.



## Key Features

### arXiv Ingestion Pipeline
- Fetches recent arXiv papers (currently focused on **2025**)
- Supports ML- and AI-relevant categories:
  - `cs.LG` — Machine Learning  
  - `stat.ML` — Statistical Machine Learning  
  - `cs.AI` — Artificial Intelligence  
  - `cs.MA` — Multi-Agent Systems
- Stores publication timestamps for **recency-aware ranking**

### Hybrid Retrieval
- Dense semantic search using **Sentence Transformers**
- Sparse keyword search using **BM25**
- Deduplication at **paper level** (not chunk level)

### Cross-Encoder Re-ranking
- Uses `cross-encoder/ms-marco-MiniLM-L-6-v2`
- Re-ranks retrieved chunks based on **query–content interaction**

### Recency-Aware Ranking
- Detects recency intent in queries (e.g. *latest*, *recent*, *state of the art*)
- Applies a **time-decay score** based on publication date

### Grounded Answer Generation
- Answers generated **only from retrieved context**
- Explicit citation of paper titles
- No hallucinated references

### Containerized Deployment
- Fully runnable via **Docker**
- Minimal setup for local execution

---

## Data Source

### arXiv Categories

The ingestion pipeline targets multiple arXiv categories that commonly host cutting-edge machine learning research.

---

## API Usage

### Requirements

To run the API, you must provide a **Groq API key**, used for **LLM-based answer generation**.

You can obtain an API key from:  
https://console.groq.com

Set the key as an environment variable:

```bash
export GROQ_API_KEY=your_api_key_here
```
